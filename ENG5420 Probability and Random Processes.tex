\documentclass[11pt]{article}
    \usepackage{caption}
    \setlength{\parindent}{0pt}
    \DeclareCaptionType{equ}[][]
    \usepackage[svgnames]{xcolor}
    
    \newcommand*{\plogo}{\fbox{$\mathcal{BM}$}}
    
    \usepackage{PTSerif}
    
    \begin{document} 
        
    \begin{titlepage}
    
        \raggedleft
        
        \vspace*{\baselineskip}
        
        {\Large Bryan Melanson}
        
        \vspace*{0.167\textheight}
        
        \textbf{\LARGE How to Not Fail}\\[\baselineskip]
        
        {\textcolor{Red}{\Huge Probability \& Random Processes}}\\[\baselineskip]
        
        {\Large \textit{While never going to class}}
        
        \vfill
        
        {\large Computer Engineering 2020 ~~\plogo}
        
        \vspace*{3\baselineskip}
    
    \end{titlepage}

    \pagebreak
    
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    \tableofcontents

    \pagebreak

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    \section{Descriptive Statistics}

    \subsection{Statistics}
    Statistics are the summarization of a set of data that has been collected, which demonstrates random variation. \textit{Extracting meaning from data.}

    \subsection{Inferential Statistics}
    Making inferences abotu a situation based on data, such as forecasting. \\
    Descriptive statistics can be the basis for inferences.

    \subsection{Representative Values}
    \begin{enumerate}
        \item{Mean} 
        \item{Median}
        \item{Mode}
        \item{Range \textit{- [Min, Max]}}
        \item{Variance \textit{- Average of deviation squared from the mean}}
        \item{Standard Deviation \textit{- Measure of average absolute deviation}}
        \item{Skewness \textit{- Measure of the shape of the distribution funciton}}
        \item{Quantiles \textit{- Generalizatino of the median to percentiles}}
    \end{enumerate}
    \subsection{Observational vs. Experimental Data}
    Experimental involves manipulation objects to determine cause and effect in data. Observational refers to naturally occurring events.
    \pagebreak

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    \section{Basic Probability}
    \subsection{Probability Calculus}
    Probability events have a total probability between zero and one.
        \begin{equ}[!ht]
            \begin{equation}
              Pr[An Event] = 1
            \end{equation}
          \caption{An event which is sure to happen}
        \end{equ} 

The definition of probability for how often an event is observed can be related to the number of repetiions of the experiment. \\


\begin{equ}[!ht]
    \begin{equation}
      Pr[Heads] =  \frac{\textrm{number } k \textrm{ of Heads in }N \textrm{ coin tosses}}{\textrm{ coin tosses}}
    \end{equation}
  \caption{Counting the probability of heads in a set of coin tosses}
\end{equ} 

The larger the number of repetiions, the higher accuracy with which we can predict the likelihood of an event happening.

\subsection{Probability Model}

\subsubsection{Events}
Events are elements in the set of possible outcomes in an experiment.

\subsubsection{Sample Space}
The set of all possible outcomes for an experiment.

\begin{equ}[!ht]
    \begin{equation}
      S = \{1,2,3,4,5,6\}
    \end{equation}
  \caption{The sample space for a dice roll}
\end{equ} 

\begin{equ}[!ht]
    \begin{equation}
      A_1 = \{1\}, A_2 = \{1,2,5\}
    \end{equation}
  \caption{Subsets containing events in the sample space}
\end{equ} 

The complement of a subset $A^C$ is the subset of all other events in the sample space which are not contained in $A$.

\subsection{Event Algebra}

\subsubsection{Or}
The combination of two or more sets.\\

For $A = \{1,2,5\}$ and $B = \{3, 4\}$, $A_1 + A_2 = \{1, 2, 3, 4, 5\}$

\subsubsection{And}
The set of events which occur in two or more sets.\\

For $A = \{1,2,3, 5\}$ and $B = \{3, 4\}$, $A_1A_2 = \{3\}$

\subsubsection*{Axioms}
\begin{enumerate}
    \item{Mutual exclusion: } $AA^c = 0$
\item{Inclusion: } $AS = A$
\item{Double complement: }$(A^C)^C = A$
\item{Commutation: } $A_1 + A_2 = A_2 + A_1$
\item{Associativity: } $A_1 + (A_2 + A_3) = (A_1 + A_2) + A_3$ 
\item{Distributivity: } $A_1 (A_2 + A_3) = A_1A_2 + A_1A_3$
\item{DeMorganâ€™s Law: } $(A_1A_2)^c = (A_1)^C + (A_2)^C$
\end{enumerate}

\subsection{Probability of Events}

\subsubsection*{Axioms}
\begin{enumerate}
    \item{For any event $A$: $Pr[A] \geq 0$} 
    \item{$Pr[S] = 1$}
    \item{If $A$ and $B$ are \textbf{Mutually Exclusive} then } $Pr[A + B] = Pr[A] + Pr[B]$
    \begin{center}
        \textbf{Mutual Exclusivity} refers to the fact that $A$ and $B$ will never occur simultaneously, ie $AB = 0$.
    \end{center}
\item{Axiom 3 can be extended: } $Pr[A_1 + A_2 + ... ] = Pr[A_1] + Pr[A_2] + ...$
\end{enumerate}

\subsubsection{Non-Mutually Exclusive}
In cases where $A$ and $B$ can occur in the same set, Axiom 3 will not apply. \\
This is due to the fact that in overlapping events, the same area of probability will be counted twice, though it has no statistical importance.

\begin{equ}[!ht]
    \begin{equation}
      Pr[A_1 + A_2] = Pr[A_1] + Pr[A_2] - Pr[A_1A_2]
    \end{equation}
  \caption{Non mutually exclusive OR}
\end{equ} 

\subsubsection{Complement of an event}
From expanding on these axioms, it can be seen that the complement of an event has a probability related to subtraction of itself from the sample space probability. If the chance of the event happening is known, the chance of an event not happening is found by subtracting this from absolute certainty.

\begin{equ}[!ht]
    \begin{equation}
      Pr[A^c] = 1 - Pr[A]
    \end{equation}
  \caption{The complement of a set versus the whole}
\end{equ} 

\subsubsection{Statistical Independence}

\textit{Two events $A$ and $B$ are said to be statiscally independent if}

\begin{equ}[!ht]
    \begin{equation}
      Pr[AB] = Pr[A]Pr[B]
    \end{equation}
  \caption{Statistical independence}
\end{equ} 

This refers to the fact that statistical data will happen independent of the preceding events. If you flip a coin, the probability of the next coin will be the same. If you take items from a bin, the probability of the next item being picked will go up, and is therefore dependent.
\subsection{Repeated Independent Trials}

From the rule of statistical independence, we can process repeated trials:
\subsubsection*{Coin Flips}
$Pr[H] = p$ \\

$Pr[T] = Pr[H^c] = 1 - p = q$ \\

$Pr[HHH] = ppp = p^3$ \\

$Pr[HTH] = pqp = p^2q$ \\


Knowing that the probability of either event is 0.5, we can take the list of possible outcomes and calculate the probability. \\


$Pr[\textit{At least two heads}] = Pr[HHH] + Pr[HHT] + Pr[HTH] + Pr[THH]$ \\ 

$ = ppp + ppq + pqp + qpp $ \\

$ = (0.5)^3 + (0.5)^2*(0.5) + (0.5)^2*(0.5) + (0.5)*(0.5)^2$ \\

$ = 0.5 $ \\

$Pr[\textit{No heads}] = Pr[TTT] = q^3 = 0.125]$

\subsubsection{Sampling With Replacement}

In this case, we consider an event where an event occurring does not subtract from a finite amount of events, ie a coin flip. In the case of heads or tails, there aren't one less heads or tails. So it is as if we replace the event in our sample space.

\subsubsection{Sampling Without Replacement}

Finite amounts of events that can be subtracted from the whole. If this event happens, it won't happen again, as if we have taken our card from a deck of cards and not placed it back in the deck.

\subsubsection{Order of Outcomes When Sampling Without Replacement}

Cases when its important what order events occur in. Did we draw the Ace of Spades within the first 3 draws? The number of each event is not important. \\

\textbf{K-Tuples} \\

When a trial is repeated $k$ times, we form a sample space of outcomes made up of $k$ number of events. \\


\textbf{The Rule of Product} \\

How many possibilities are there for the formation of $k$-tuples, if there are $N_i$ choices for the $i$th  element? \\

\begin{equ}[!ht]
    \begin{equation}
        N_1 * N_2 * ... N_k
    \end{equation}
  \caption{The Rule of Product}
\end{equ} 

By this rule, the number of possibilities when rolling a dice, then flipping a coin, will be $6 * 2$. In the case where same number of outcomes are possible with each experiment, the number of $k$-tuples is $N^k$, or $2^k$ in the case of repeated coin tosses.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}